{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Desarrollo de redes neuronales\n",
    "Objetivos: \n",
    "1. Identificar columnas numéricas y columnas categóricas.\n",
    "2. Diseñar una red de neuronas para resolver un problema de **regresión**.\n",
    "3. Utilizar métricas para evaluar el modelo generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Datos ganaderos de Catalunya\n",
    "Se trabajará únicamente con las columnas: [CODI POSTAL EXPLOTACIO,SERVEI TERRITORIAL  EXPLOTACIÓ,PROVINCIA EXPLOTACIÓ,COMARCA EXPLOTACIÓ,MUNICIPI EXPLOTACIÓ,TIPUS EXPLOTACIÓ,ESPÈCIE, INTEGRADORA, NOM ADS,CLASSIFICACIÓ ZOOTÈCNICA,DATA CLASSIFICACIÓ ZOOTÈCNICA,SISTEMA PRODUCTIU,CRITERI DE SOSTENIBILITAT,CAPACITAT PRODUCTIVA,TOTAL CAP PONEDORES,TOTAL URM,TOTAL NITROGEN].\n",
    "\n",
    "El modelo a generar debe trabajar con TOTAL_URM o TOTAL_NITROGEN como variable a predecir. A elección del grupo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('carga_ganadera.csv')\n",
    "\n",
    "columnas = [\n",
    "    'CODI POSTAL EXPLOTACIO', 'SERVEI TERRITORIAL  EXPLOTACIÓ', 'PROVINCIA EXPLOTACIÓ',\n",
    "    'COMARCA EXPLOTACIÓ', 'MUNICIPI EXPLOTACIÓ', 'TIPUS EXPLOTACIÓ', 'ESPÈCIE', \n",
    "    'INTEGRADORA', 'NOM ADS', 'CLASSIFICACIÓ ZOOTÈCNICA', 'DATA CLASSIFICACIÓ ZOOTÈCNICA',\n",
    "    'SISTEMA PRODUCTIU', 'CRITERI DE SOSTENIBILITAT', 'CAPACITAT PRODUCTIVA', \n",
    "    'TOTAL CAP PONEDORES', 'TOTAL URM', 'TOTAL NITROGEN'\n",
    "]\n",
    "\n",
    "data = data[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.1 Conversión a One-Hot Encoding**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1.1 Convertir las columnas categóricas a one-hot encoding. (1 ptos)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "x_categorical = data[categorical_columns]\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "x_encoded = encoder.fit_transform(x_categorical)\n",
    "\n",
    "categorical_encoded_columns = [\n",
    "    f'{col}_{cat}' for i, col in enumerate(x_categorical.columns) for cat in encoder.categories_[i]\n",
    "]\n",
    "\n",
    "x_encoded_df = pd.DataFrame(x_encoded.toarray(), columns=categorical_encoded_columns)\n",
    "df = pd.concat([data.drop(columns=categorical_columns), x_encoded_df], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.2 Red Neuronal para TOTAL_URM  -  random_state=42**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos esta sentencia de código para ver que columnas tenian NaN, lo cual no dejaba ejecutar la función de activación de sigmoid\n",
    "\n",
    "``nan_columns = x_train.columns[x_train.isnull().any()]``  \n",
    "``print('Columnas con NaN en x_train:', nan_columns)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1.2 Construir una red de neuronas para identificar el `TOTAL_URM` o `TOTAL_NITROGEN`. (3.5 ptos) \n",
    "# Se evaluará el loss function utilizado en cada capa.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(\n",
    "    42\n",
    ")\n",
    "\n",
    "target_variable = 'TOTAL URM'\n",
    "\n",
    "x = df.drop(columns=[target_variable])\n",
    "y = df[target_variable]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('NaN en x_train:', x_train.isnull().any().any())\n",
    "print('NaN en x_test:', x_test.isnull().any().any())\n",
    "\n",
    "# Manejar NaN - Imputación con la media\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(x_train)\n",
    "\n",
    "x_train_imputed = pd.DataFrame(imputer.transform(x_train), columns=x_train.columns, index=x_train.index)\n",
    "x_test_imputed = pd.DataFrame(imputer.transform(x_test), columns=x_test.columns, index=x_test.index)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_imputed)\n",
    "x_test_scaled = scaler.transform(x_test_imputed)\n",
    "\n",
    "print('NaN en x_train:', x_train_imputed.isnull().any().any())\n",
    "print('NaN en x_test:', x_test_imputed.isnull().any().any())\n",
    "\n",
    "model_relu = keras.models.Sequential()\n",
    "model_relu.add(keras.layers.InputLayer(shape=[x_train_scaled.shape[1]])) \n",
    "model_relu.add(keras.layers.Dense(200, activation=\"relu\"))\n",
    "model_relu.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model_relu.add(keras.layers.Dense(50, activation=\"relu\"))\n",
    "model_relu.add(keras.layers.Dense(20, activation=\"relu\"))\n",
    "model_relu.add(keras.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "model_sigmoid = keras.models.Sequential()\n",
    "model_sigmoid.add(keras.layers.InputLayer(shape=[x_train_scaled.shape[1]])) \n",
    "model_sigmoid.add(keras.layers.Dense(200, activation=\"sigmoid\"))\n",
    "model_sigmoid.add(keras.layers.Dense(100, activation=\"sigmoid\"))\n",
    "model_sigmoid.add(keras.layers.Dense(50, activation=\"sigmoid\"))\n",
    "model_sigmoid.add(keras.layers.Dense(20, activation=\"sigmoid\"))\n",
    "model_sigmoid.add(keras.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "model_tanh = keras.models.Sequential()\n",
    "model_tanh.add(keras.layers.InputLayer(shape=[x_train_scaled.shape[1]])) \n",
    "model_tanh.add(keras.layers.Dense(200, activation=\"tanh\"))\n",
    "model_tanh.add(keras.layers.Dense(100, activation=\"tanh\"))\n",
    "model_tanh.add(keras.layers.Dense(50, activation=\"tanh\"))\n",
    "model_tanh.add(keras.layers.Dense(20, activation=\"tanh\"))\n",
    "model_tanh.add(keras.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "model_elu = keras.models.Sequential()\n",
    "model_elu.add(keras.layers.InputLayer(shape=[x_train_scaled.shape[1]])) \n",
    "model_elu.add(keras.layers.Dense(200, activation=\"elu\"))\n",
    "model_elu.add(keras.layers.Dense(100, activation=\"elu\"))\n",
    "model_elu.add(keras.layers.Dense(50, activation=\"elu\"))\n",
    "model_elu.add(keras.layers.Dense(20, activation=\"elu\"))\n",
    "model_elu.add(keras.layers.Dense(1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.3 Evaluación de desenvolvimiento del modelo - 50 épocas**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilamos ambos modelos con métricas\n",
    "\n",
    "model_relu.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "\n",
    "model_sigmoid.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "\n",
    "model_tanh.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "\n",
    "model_elu.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_relu = model_relu.fit(\n",
    "    x_train_scaled, y_train,\n",
    "    validation_data=(x_test_scaled, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_sigmoid = model_sigmoid.fit(\n",
    "    x_train_scaled, y_train,\n",
    "    validation_data=(x_test_scaled, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_tanh = model_tanh.fit(\n",
    "    x_train_scaled, y_train,\n",
    "    validation_data=(x_test_scaled, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_elu = model_elu.fit(\n",
    "    x_train_scaled, y_train,\n",
    "    validation_data=(x_test_scaled, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_elu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu\n",
    "\n",
    "history_dict = history_relu.history\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dict['mean_absolute_error'], label='Training MAE', color='blue')\n",
    "plt.plot(history_dict['val_mean_absolute_error'], label='Validation MAE', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "\n",
    "history_dict2 = history_sigmoid.history\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict2['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history_dict2['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dict2['mean_absolute_error'], label='Training MAE', color='blue')\n",
    "plt.plot(history_dict2['val_mean_absolute_error'], label='Validation MAE', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh\n",
    "\n",
    "history_dict3 = history_tanh.history\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict3['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history_dict3['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dict3['mean_absolute_error'], label='Training MAE', color='blue')\n",
    "plt.plot(history_dict3['val_mean_absolute_error'], label='Validation MAE', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elu\n",
    "\n",
    "history_dict4 = history_elu.history\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict4['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history_dict4['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dict4['mean_absolute_error'], label='Training MAE', color='blue')\n",
    "plt.plot(history_dict4['val_mean_absolute_error'], label='Validation MAE', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "y_pred_relu = model_relu.predict(x_test_scaled)\n",
    "y_pred_sigmoid = model_sigmoid.predict(x_test_scaled)\n",
    "y_pred_tanh = model_tanh.predict(x_test_scaled)\n",
    "y_pred_elu = model_elu.predict(x_test_scaled)\n",
    "\n",
    "# Calcular métricas para el modelo con ReLU\n",
    "mse_relu = mean_squared_error(y_test, y_pred_relu)\n",
    "mae_relu = mean_absolute_error(y_test, y_pred_relu)\n",
    "r2_relu = r2_score(y_test, y_pred_relu)\n",
    "\n",
    "print(\"\\nMétricas para el modelo con ReLU:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_relu:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_relu:.2f}\")\n",
    "print(f\"R² Score: {r2_relu * 100:.2f}%\")\n",
    "\n",
    "# Calcular métricas para el modelo con Sigmoid\n",
    "mse_sigmoid = mean_squared_error(y_test, y_pred_sigmoid)\n",
    "mae_sigmoid = mean_absolute_error(y_test, y_pred_sigmoid)\n",
    "r2_sigmoid = r2_score(y_test, y_pred_sigmoid)\n",
    "\n",
    "print(\"\\nMétricas para el modelo con Sigmoid:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_sigmoid:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_sigmoid:.2f}\")\n",
    "print(f\"R² Score: {r2_sigmoid * 100:.2f}%\")\n",
    "\n",
    "# Calcular métricas para el modelo con Tanh\n",
    "mse_tanh = mean_squared_error(y_test, y_pred_tanh)\n",
    "mae_tanh = mean_absolute_error(y_test, y_pred_tanh)\n",
    "r2_tanh = r2_score(y_test, y_pred_tanh)\n",
    "\n",
    "print(\"\\nMétricas para el modelo con Sigmoid:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_tanh:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_tanh:.2f}\")\n",
    "print(f\"R² Score: {r2_tanh * 100:.2f}%\")\n",
    "\n",
    "# Calcular métricas para el modelo con Elu\n",
    "mse_elu = mean_squared_error(y_test, y_pred_elu)\n",
    "mae_elu = mean_absolute_error(y_test, y_pred_elu)\n",
    "r2_elu = r2_score(y_test, y_pred_elu)\n",
    "\n",
    "print(\"\\nMétricas para el modelo con Sigmoid:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_elu:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_elu:.2f}\")\n",
    "print(f\"R² Score: {r2_elu * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Gráfico de dispersión para el modelo con ReLU\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(y_test, y_pred_relu, alpha=0.5, color='blue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Valores Predichos')\n",
    "plt.title('Modelo con ReLU')\n",
    "\n",
    "# Gráfico de dispersión para el modelo con Sigmoid\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred_sigmoid, alpha=0.5, color='orange')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Valores Predichos')\n",
    "plt.title('Modelo con Sigmoid')\n",
    "\n",
    "# Gráfico de dispersión para el modelo con Tanh\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(y_test, y_pred_tanh, alpha=0.5, color='green')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Valores Predichos')\n",
    "plt.title('Modelo con Tanh')\n",
    "\n",
    "# Gráfico de dispersión para el modelo con Elu\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(y_test, y_pred_elu, alpha=0.5, color='red')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Valores Predichos')\n",
    "plt.title('Modelo con Elu')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "residuals_relu = y_test - y_pred_relu.flatten()\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(y_pred_relu, residuals_relu, alpha=0.5, color='blue')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.xlabel('Valores Predichos')\n",
    "plt.ylabel('Residuales')\n",
    "plt.title('Residuales para el Modelo con ReLU')\n",
    "\n",
    "residuals_sigmoid = y_test - y_pred_sigmoid.flatten()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_pred_sigmoid, residuals_sigmoid, alpha=0.5, color='orange')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.xlabel('Valores Predichos')\n",
    "plt.ylabel('Residuales')\n",
    "plt.title('Residuales para el Modelo con Sigmoid')\n",
    "\n",
    "residuals_tanh = y_test - y_pred_tanh.flatten()\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(y_pred_tanh, residuals_tanh, alpha=0.5, color='green')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.xlabel('Valores Predichos')\n",
    "plt.ylabel('Residuales')\n",
    "plt.title('Residuales para el Modelo con Tanh')\n",
    "\n",
    "residuals_elu = y_test - y_pred_elu.flatten()\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(y_pred_elu, residuals_elu, alpha=0.5, color='green')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.xlabel('Valores Predichos')\n",
    "plt.ylabel('Residuales')\n",
    "plt.title('Residuales para el Modelo con Elu')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.4 Análisis de resultados**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Datos jugadores del FIFA\n",
    "El dataset en la actividad incluye información sobre la performance de cada jugador en el juego FIFA. \n",
    "\n",
    "1. Tratamiento de los datos. Cada jugador tiene una posición determinada `team_position` y según la columna que tiene el mismo valor, tiene un puntaje. (0.5 pts)\n",
    "   1. De acuerdo a ese puntaje, asignar las siguientes categorías:\n",
    "      * Poor:[46.0, 62.0]\n",
    "      * Interm:[63.0, 66.0]\t\n",
    "      * Good:[67.0, 71.0]\t\n",
    "      * Excel:[72.0, 94.0]   \n",
    "2. Generación del modelo de clasificación, tomando en cuenta esta nueva columna a partir de las columnas (2.5 pts)\n",
    "   * Considera las siguientes columnas como features: `attacking_crossing\tattacking_finishing\tattacking_heading_accuracy\tattacking_short_passing\tattacking_volleys\tskill_dribbling\tskill_curve\tskill_fk_accuracy\tskill_long_passing\tskill_ball_control\tmovement_acceleration\tmovement_sprint_speed\tmovement_agility\tmovement_reactions\tmovement_balance\tpower_shot_power\tpower_jumping\tpower_stamina\tpower_strength\tpower_long_shots\tmentality_aggression\tmentality_interceptions\tmentality_positioning\tmentality_vision\tmentality_penalties\tmentality_composure\tdefending_marking\tdefending_standing_tackle\tdefending_sliding_tackle\tgoalkeeping_diving\tgoalkeeping_handling\tgoalkeeping_kicking\tgoalkeeping_positioning\tgoalkeeping_reflexes`. \n",
    "3. Generar el análisis y gráficas para analizar la performance del modelo. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_players = pd.read_csv('players_20.csv')\n",
    "data_players.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_players['team_position'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.1 Tratamiento de datos**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_players['team_position'] = data_players['team_position'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos los que tengan NA \n",
    "columnas_a_verificar = ['rw', 'lw', 'cam', 'rcm', 'lcb', 'st', 'cdm', 'ldm', 'rm',\n",
    "                        'rcb', 'lcm', 'lm', 'cf', 'lb', 'ls', 'rb', 'rdm',\n",
    "                        'ram', 'rs', 'rf', 'cm', 'cb', 'lf', 'lam', 'rwb', 'lwb']\n",
    "\n",
    "data_players = data_players.dropna(subset=columnas_a_verificar)\n",
    "\n",
    "# Nos quedamos solo con los números antes del \"+\"\n",
    "def extract_main_score(score):\n",
    "    try:\n",
    "        if pd.notna(score) and '+' in str(score):\n",
    "            return int(str(score).split('+')[0])\n",
    "        elif pd.notna(score):\n",
    "            return int(score)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "for columna in columnas_a_verificar:\n",
    "    data_players[columna] = data_players[columna].apply(extract_main_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_players.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_players = data_players[data_players['team_position'].isin(columnas_a_verificar)]\n",
    "\n",
    "def get_score_from_position(row):\n",
    "    position = row['team_position']\n",
    "    if position in data_players.columns and pd.notna(row[position]):\n",
    "        return row[position]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "data_players['score_from_position'] = data_players.apply(get_score_from_position, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_performance(score):\n",
    "    if 46 <= score <= 62:\n",
    "        return 'Poor'\n",
    "    elif 63 <= score <= 66:\n",
    "        return 'Interm'\n",
    "    elif 67 <= score <= 71:\n",
    "        return 'Good'\n",
    "    elif 72 <= score <= 94:\n",
    "        return 'Excel'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Aplicamos la función para crear la nueva columna categorizada\n",
    "data_players['categoria_segun_position'] = data_players['score_from_position'].apply(categorize_performance)\n",
    "\n",
    "# Eliminamos las filas con la categoría 'Unknown' (si existieran)\n",
    "data_players = data_players[data_players['categoria_segun_position'] != 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_players['categoria_segun_position'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.2 Modelo de clasificación**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'attacking_crossing', 'attacking_finishing', 'attacking_heading_accuracy',\n",
    "    'attacking_short_passing', 'attacking_volleys', 'skill_dribbling',\n",
    "    'skill_curve', 'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control',\n",
    "    'movement_acceleration', 'movement_sprint_speed', 'movement_agility',\n",
    "    'movement_reactions', 'movement_balance', 'power_shot_power',\n",
    "    'power_jumping', 'power_stamina', 'power_strength', 'power_long_shots',\n",
    "    'mentality_aggression', 'mentality_interceptions', 'mentality_positioning',\n",
    "    'mentality_vision', 'mentality_penalties', 'mentality_composure',\n",
    "    'defending_marking', 'defending_standing_tackle', 'defending_sliding_tackle',\n",
    "    'goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking',\n",
    "    'goalkeeping_positioning', 'goalkeeping_reflexes'\n",
    "]\n",
    "\n",
    "x_players = data_players[features]\n",
    "y_players = data_players['categoria_segun_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(\n",
    "    42\n",
    ")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_players)\n",
    "\n",
    "x_train_players, x_test_players, y_train_players, y_test_players = train_test_split(x_players, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "input_dim = x_players.shape[1]\n",
    "\n",
    "model_players = keras.models.Sequential()\n",
    "model_players.add(keras.layers.Input(shape=(input_dim,)))\n",
    "\n",
    "for _ in range(10):\n",
    "    model_players.add(keras.layers.Dense(100, activation='relu'))\n",
    "model_players.add(keras.layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_players.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cat_players = model_players.fit(\n",
    "    x_train_players, \n",
    "    y_train_players, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_data=(x_test_players, y_test_players)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_players.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model_players.predict(x_test_players)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "report = classification_report(y_test_players, y_pred_classes, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_players = history_cat_players.history\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_players['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history_players['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_players['accuracy'], label='Training Accuracy', color='blue')\n",
    "plt.plot(history_players['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_players = history_cat_players.history\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_players['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history_players['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_players['accuracy'], label='Training Accuracy', color='blue')\n",
    "plt.plot(history_players['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_players, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.3 Análisis de graficos de la performance del modelo**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.4 Análisis de resultados**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Índice latinoamericano de IA\n",
    "Revisar el índide Latinoamericano de Inteligencia Artificial https://indicelatam.cl/. También disponible en PDF en el blackboard.\n",
    "Responder a las siguientes preguntas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Cuál es el objetivo principal del Índice Latinoamericano de IA y cómo se mide el progreso en la región? **(0.5 pts)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> El ILIA tiene como objetivo ayudar a que la Inteligencia Artificial se desarrolle de manera más inclusiva en América Latina. Busca aprovechar las oportunidades, identificar las diferencias que aún existen, y ayudar a impulsar acciones concretas para que la IA avance de manera beneficiosa para todos en la región. Para medir cómo vamos, el índice usa más de 70 indicadores, organizados en dimensiones como Factores Habilitantes, Investigación, Desarrollo y Gobernanza. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ¿Qué países de Latinoamérica lideran actualmente el ranking del Índice Latinoamericano de IA y por qué? **(0.5 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En el ILIA 2024, los países que están liderando el camino son Chile, Brasil y Uruguay. Lo que los hace destacar es su trabajo en diferentes áreas clave, como tener una buena infraestructura tecnológica, fomentar el talento especializado, promover la ciencia y la innovación. Estos países se encuentran en lo que se llama el \"Cuadrante I\", lo que básicamente significa que tienen un entorno bastante favorable para el desarrollo de la IA y, por eso, están bien posicionados para ser líderes regionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Cuáles son los principales desafíos que enfrenta la región para avanzar en la adopción y desarrollo de la Inteligencia Artificial? **(0.5 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Algunos de los grandes problemas que enfrenta la región para desarrollar la IA incluyen la falta de talento especializado y la dificultad para retener a los expertos que se forman, ya que muchos terminan yéndose a otros países. Además, falta infraestructura adecuada, como computadoras de alto rendimiento y acceso a redes rápidas. También hay una gran brecha de género, lo que significa que las mujeres aún están subrepresentadas tanto en los estudios como en el uso de la IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿De qué manera el Índice Latinoamericano de IA considera la ética y la responsabilidad social en el desarrollo de la IA? **(0.5 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> El índice también se preocupa por el aspecto ético y la responsabilidad social en el desarrollo de la IA. Evalúa qué tan bien los países están integrando estrategias de IA que respeten estos valores, incluyendo marcos regulatorios para asegurarse de que todo se haga de manera responsable. Por ejemplo, menciona la Metodología RAM de la UNESCO, que sirve como una guía para lograr una IA más ética. Además, hay iniciativas conjuntas con universidades para crear algoritmos más transparentes y justos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ¿Qué iniciativas gubernamentales y privadas están impulsando el desarrollo de la IA en Latinoamérica y cómo se reflejan en el Índice Latinoamericano de IA? **(0.5 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En América Latina, hay iniciativas tanto del gobierno como del sector privado para apoyar el desarrollo de la IA. Algunos ejemplos incluyen la inversión en infraestructura tecnológica con apoyo de la CAF y la creación del Grupo de Trabajo por la Ética de la IA, que nació durante la Cumbre de Santiago. También hay programas del BID que buscan ayudar a que más empresas adopten la IA. A nivel privado, grandes empresas como Google, Microsoft y AWS están poniendo su grano de arena para que el desarrollo de la IA en la región sea más equilibrado y responsable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. De acuerdo al índice latinoamericano de IA ¿Cuál es el potencial del desarrollo de la inteligencia artificial en el Perú? **(0.5 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Perú tiene un gran potencial, pero enfrenta algunos desafíos importantes. En el índice, Perú está en el \"Cuadrante III\" en lo que respecta a infraestructura y desarrollo de la IA, lo que significa que hay mucho por mejorar. Sin embargo, en el aspecto de gobernanza, está un poco mejor, en el \"Cuadrante II\", lo que indica que hay un buen ambiente regulatorio. Si Perú logra mejorar en las áreas donde hay más problemas, podría aprovechar mucho mejor las oportunidades que trae la IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ¿Qué iniciativas puede tomar en el Perú para ascender en el índice latinoamericano? **(1 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Para que Perú pueda avanzar en el Índice Latinoamericano de Inteligencia Artificial, lo primero es ponerle atención a la infraestructura tecnológica: mejorar la conectividad, apostar por el 5G y contar con mejor capacidad de computación. Sin esto, no se puede competir. Además, hay que trabajar en formar y retener talento especializado. No basta con educar, también hay que hacer atractivo quedarse en el país. Es clave tener acceso a datos de calidad, y para eso se necesita infraestructura que los respalde y políticas que aseguren su uso responsable. También necesitamos marcos regulatorios que promuevan la IA de forma ética, apoyando tanto la inversión pública como la privada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Historia de la Inteligencia Artificial\n",
    "\n",
    "**Objetivo:** Extraer 4 oraciones clave y 3 ideas (del grupo) sobre el capítulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Capítulo 1\n",
    "   1. **\"Turing was, for all practical purposes, the inventor of the computer, and shortly after that, he largely invented the field of AI.\"**\n",
    "   - \"Turing fue, a todos los efectos prácticos, el inventor de la computadora, y poco después de eso, inventó en gran medida el campo de la IA\"\n",
    "   2. **\"And if all mathematical decision problems can be solved by following a recipe, then for any decision problem, you should be able to design a Turing machine to solve it.\"**\n",
    "   - \"Y si todos los problemas de decisión matemática se pueden resolver siguiendo una receta, entonces para cualquier problema de decisión, debería ser posible diseñar una máquina de Turing para resolverlo\"\n",
    "   3. **\"The question of whether AI is possible ultimately amounts to whether we can produce intelligent behavior simply by following lists of instructions like these.\"**\n",
    "   - La cuestión de si la IA es posible, en última instancia, se reduce a si podemos producir un comportamiento inteligente simplemente siguiendo listas de instrucciones como estas\"\n",
    "   4. **\"Although computers are just machines for following instructions, this does not mean that they are incapable of making decisions.\"**\n",
    "   - \"Aunque las computadoras son simplemente máquinas para seguir instrucciones, esto no significa que sean incapaces de tomar decisiones\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Capítulo 2\n",
    "   1. **\"The next two decades were the first boom in AI. There was a flush of optimism, growth, and apparent progress, leading to the era called the golden age of AI, from about 1956 to 1974.\"**\n",
    "   - \"Las siguientes dos décadas fueron el primer auge de la IA. Hubo un auge de optimismo, crecimiento y aparente progreso, que condujo a la era llamada la edad de oro de la IA, desde aproximadamente 1956 hasta 1974.\"\n",
    "   2. **\"Instead of starting out trying to build a complete general intelligent system, the approach adopted was to identify the various different individual capabilities that seemed to be required for general-purpose AI and to build systems that could demonstrate these capabilities.\"**\n",
    "   - \"En lugar de comenzar a intentar construir un sistema inteligente general completo, el enfoque adoptado fue identificar las distintas capacidades individuales que parecían necesarias para una IA de propósito general y construir sistemas que pudieran demostrar esas capacidades.\"\n",
    "   3. **\"There was a good deal of naivety in the golden age, with researchers making reckless and grandiose predictions about the likely speed of progress in the field, which have haunted AI ever since.\"**\n",
    "   - \"Hubo mucha ingenuidad en la edad de oro, con investigadores haciendo predicciones imprudentes y grandiosas sobre la posible velocidad de progreso en el campo, lo que ha perseguido a la IA desde entonces.\"\n",
    "   4. **\"The decade from the early 1970s to the early 1980s later became known as the AI winter, although it should perhaps better be known as the first AI winter, because there were more to come\"**\n",
    "   - \"La década que va desde principios de los años 1970 hasta principios de los años 1980 se conoció más tarde como el invierno de la IA, aunque tal vez debería llamarse mejor el primer invierno de la IA, porque aún quedaban más por venir.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Capítulo 6\n",
    "   1. **\" In the second decade of the twenty-first century, AI has attracted more interest than any new technology since the World Wide Web in the 1990s.\"**\n",
    "   - \"En la segunda década del siglo XXI, la IA ha atraído más interés que cualquier otra tecnología nueva desde la aparición de la World Wide Web en los años 90.\"\n",
    "   2. **\" Everywhere that technology is used, AI is finding applications: in education, science, industry, commerce, agriculture, health care, entertainment, the media and arts, and beyond.\"**\n",
    "   - \"En todos los lugares donde se utiliza la tecnología, la IA está encontrando aplicaciones: en la educación, la ciencia, la industria, el comercio, la agricultura, la atención sanitaria, el entretenimiento, los medios de comunicación y las artes, y más allá.\"\n",
    "   3. **\"One important new opportunity for AI-powered health care is what we might call personal health care management.\"**\n",
    "   - \"Una nueva e importante oportunidad para la atención sanitaria basada en inteligencia artificial es lo que podríamos llamar la gestión de la atención sanitaria personal.\"\n",
    "   4. **\"If you could find a way for a car to know precisely where it was and what was around it, then you would have solved the problem of driverless cars. The solution to this problem was to be modern machine learning techniques: without them, driverless cars would not be possible\"**\n",
    "   - \"Si se pudiera encontrar una manera de que un coche supiera exactamente dónde se encuentra y qué hay a su alrededor, se habría solucionado el problema de los coches sin conductor. La solución a este problema eran las técnicas modernas de aprendizaje automático: sin ellas, los coches sin conductor no serían posibles.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ideas (del grupo)\n",
    "   1. Alan Turing, con su tería de las máquinas que siguen instrucciones (Máquinas de Turing), creó las bases de las computadoras actuales y de la IA. Los medios introducieron el término \"cerebros electrónicos\" para describir los primeros ordenadores, lo que dio la impresión errónea de que estas máquinas poseían algún tipo de inteligencia superior. Sin embargo, los ordenadores no eran inteligentes; solo seguían instrucciones, llamadas algoritmos.\n",
    "   2. En la época dorada de la inteligencia artifial se realizaron muchos avances y se tenia muchas espectativas muy poco realistas de lo que sería capaz de realizar esta tecnologia en el contexto de la época. Sin embargo, pronto se encotraron con barreras debido a las limitaciones tecnologias y debido a criticas fue perdiendo la inversión con la que contaba inicialmente.\n",
    "   3. El deep learning ha permitido que la inteligencia artificial encuentre aplicaciones en casi todos los aspectos de la vida, desde la medicina hasta la creación de imágenes y la conducción autónoma."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
